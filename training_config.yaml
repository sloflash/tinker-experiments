# Training Configuration for Prompt Distillation
# Student Model: Llama-3.2-1B learning Beethoven character without 591-word prompt

# Model Configuration
model:
  base_model: "Qwen/Qwen3-4B-Instruct-2507"  # Small student model (Tinker supported)
  lora_rank: 32                              # LoRA rank (per tinker-api quick-ref)
  lora_alpha: 64                             # LoRA alpha (2x rank is common)
  lora_dropout: 0.05                         # Dropout for regularization

# Training Hyperparameters
training:
  learning_rate: 1.0e-4                   # Standard LR for LoRA fine-tuning
  num_steps: 1000                         # Full training for large dataset
  batch_size: 1                           # Process 1 example per step
  gradient_accumulation_steps: 1          # No accumulation
  warmup_steps: 100                       # 10% warmup

  # Optimizer settings (Adam)
  optimizer: "adam"
  beta1: 0.9
  beta2: 0.999
  eps: 1.0e-8

  # Regularization
  weight_decay: 0.01
  max_grad_norm: 1.0

# Data Configuration
data:
  train_file: "train.jsonl"
  val_file: "val.jsonl"
  max_seq_length: 2048                    # Max context length

# Checkpointing & Logging
checkpointing:
  save_every: 200                         # Save every 200 steps
  output_dir: "checkpoints/"
  keep_last_n: 5                          # Keep last 5 checkpoints

logging:
  log_every: 10                           # Log every 10 steps
  eval_every: 100                         # Evaluate every 100 steps
  log_dir: "training_logs/"

# Metadata
metadata:
  experiment_name: "beethoven_prompt_distillation"
  description: "Distilling 591-word Beethoven character prompt into Llama-3.2-1B"
  teacher_model: "Qwen/Qwen3-30B-A3B"
  prompt_tokens_saved: 591
  dataset_size: 5000
